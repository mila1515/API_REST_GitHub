"""
extract_users.py

Script d'extraction d'utilisateurs GitHub :
- Se connecte Ã  l'API GitHub avec un token personnel.
- RÃ©cupÃ¨re des utilisateurs par pagination, en respectant les quotas de l'API.
- Pour chaque utilisateur, rÃ©cupÃ¨re les dÃ©tails (login, id, avatar, date de crÃ©ation, bio).
- Filtre les utilisateurs selon plusieurs critÃ¨res (date, avatar, bio).
- Sauvegarde le rÃ©sultat dans data/users.json.

Usage :
    python extract_users.py --max-users 60

PrÃ©requis :
    - DÃ©pendances Python (voir requirements.txt)
    - Fichier .env avec GITHUB_TOKEN
"""

# ==== Import des bibliothÃ¨ques nÃ©cessaires ====

import requests
import os
import time
import json
from datetime import datetime
from dotenv import load_dotenv
import argparse

# ==== Ã‰tape 2 : Chargement du token GitHub depuis .env ====
load_dotenv()
GITHUB_TOKEN = os.getenv("GITHUB_TOKEN")
if not GITHUB_TOKEN:
    print("[âŒ] Token GitHub non trouvÃ© dans .env (GITHUB_TOKEN).")
    exit(1)

# PrÃ©paration des headers pour l'authentification Ã  l'API GitHub
HEADERS = {
    "Authorization": f"token {GITHUB_TOKEN}",
    "Accept": "application/vnd.github.v3+json"
}

BASE_URL = "https://api.github.com"
USERS_ENDPOINT = f"{BASE_URL}/users"

# === CrÃ©ation dossier pour sauvegarder les donnÃ©es ===
os.makedirs("data", exist_ok=True)

# Fonction pour gÃ©rer le quota de l'API GitHub (attend si quota Ã©puisÃ©)
def handle_rate_limit(response):
    """
    Ã‰tape 3 : Gestion des quotas de l'API GitHub.
    Pause si quota Ã©puisÃ©, en fonction des headers.
    """
    remaining = int(response.headers.get("X-RateLimit-Remaining", 1))
    reset_time = int(response.headers.get("X-RateLimit-Reset", time.time()))
    if remaining == 0:
        wait_time = reset_time - time.time()
        if wait_time > 0:
            print(f"[â³] Quota API GitHub Ã©puisÃ©. Pause jusqu'Ã  {datetime.fromtimestamp(reset_time)} ({int(wait_time)}s)...")
            time.sleep(wait_time + 5)

# Fonction pour rÃ©cupÃ©rer les dÃ©tails d'un utilisateur (login, id, avatar, etc.)
def get_user_details(login):
    """
    Ã‰tape 1 : RÃ©cupÃ©rer les infos dÃ©taillÃ©es d'un utilisateur via /users/<login>.
    Renvoie un dict avec login, id, avatar_url, created_at, bio ou None si erreur.
    """
    url = f"{USERS_ENDPOINT}/{login}"
    try:
        response = requests.get(url, headers=HEADERS)
        handle_rate_limit(response)

        if response.status_code == 200:
            data = response.json()
            return {
                "login": data["login"],
                "id": data["id"],
                "created_at": data["created_at"],
                "avatar_url": data["avatar_url"],
                "bio": data.get("bio")
            }
        elif response.status_code == 403:
            print(f"[âš ï¸] 403 Forbidden - quota ou token ? Pause 60s...")
            time.sleep(60)
        elif response.status_code == 429:
            print(f"[âš ï¸] 429 Too Many Requests - temporisation progressive...")
            time.sleep(10)
        elif 500 <= response.status_code < 600:
            print(f"[âš ï¸] Erreur serveur {response.status_code}, nouvelle tentative dans 5s...")
            time.sleep(5)
        else:
            print(f"[âš ï¸] Erreur HTTP {response.status_code} pour utilisateur {login}")
    except Exception as e:
        print(f"[âŒ] Exception lors de rÃ©cupÃ©ration de {login} : {e}")
    return None

# Point de dÃ©part pour la pagination (ID utilisateur)
START_SINCE_ID = 30000000

def extract_users(max_users):
    """
    Ã‰tape 4 & 5 : Pagination automatique et gestion des erreurs.
    - RÃ©cupÃ¨re max_users utilisateurs en paginant via since=<id>.
    - Filtre utilisateurs crÃ©Ã©s aprÃ¨s 2015, avec avatar_url non vide et bio renseignÃ©e.
    """
    users = []
    since_id = START_SINCE_ID
    no_new_user_count = 0  # compteur batches sans ajout utilisateur

    while len(users) < max_users:
        print(f"[ğŸ”„] RequÃªte utilisateurs depuis ID {since_id}...")
        try:
            # RÃ©cupÃ©ration d'un batch d'utilisateurs (pagination)
            response = requests.get(f"{USERS_ENDPOINT}?since={since_id}", headers=HEADERS)
            handle_rate_limit(response)

            if response.status_code != 200:
                print(f"[âš ï¸] Erreur HTTP {response.status_code} sur la requÃªte principale, pause 5s...")
                time.sleep(5)
                continue

            batch = response.json()
            if not batch:
                print("[âš ï¸] Aucun utilisateur reÃ§u, fin de pagination.")
                break

            new_users_in_batch = 0

            for user in batch:
                if len(users) >= max_users:
                    break

                # RÃ©cupÃ©rer dÃ©tails complets utilisateur
                details = get_user_details(user["login"])
                if not details:
                    continue

                # Filtrer selon les critÃ¨res demandÃ©s :
                # - created_at aprÃ¨s 2015-01-01
                # - avatar_url non vide
                # - bio renseignÃ©e (pas None, pas vide)
                cutoff_date = datetime(2015, 1, 1)
                created_date = datetime.strptime(details["created_at"], "%Y-%m-%dT%H:%M:%SZ")
                if created_date < cutoff_date:
                    print(f"[â©] {details['login']} ignorÃ© (crÃ©Ã© le {created_date.date()}, avant 2015)")
                    continue

                if not details["avatar_url"]:
                    print(f"[â©] {details['login']} ignorÃ© (avatar_url vide)")
                    continue

                bio = details.get("bio")
                if bio is None or bio.strip() == "":
                    print(f"[â©] {details['login']} ignorÃ© (bio vide ou non renseignÃ©e)")
                    continue

                # Si passe tous les filtres, ajout Ã  la liste
                users.append(details)
                new_users_in_batch += 1
                print(f"[âœ”ï¸] {details['login']} ajoutÃ©.")

            if new_users_in_batch == 0:
                no_new_user_count += 1
                print(f"[âš ï¸] Aucun nouvel utilisateur ajoutÃ© dans ce batch ({no_new_user_count} fois de suite).")
                if no_new_user_count >= 5:
                    print("[âš ï¸] ArrÃªt : trop de batches sans ajout.")
                    break
            else:
                no_new_user_count = 0

            # PrÃ©parer la prochaine pagination depuis le dernier ID de batch
            since_id = batch[-1]["id"]
            time.sleep(1)  # Petite pause pour ne pas spammer l'API

        except Exception as e:
            print(f"[âŒ] Exception lors de la requÃªte principale : {e}")
            time.sleep(5)

    return users

# Fonction pour sauvegarder les utilisateurs extraits dans un fichier JSON
def save_users_to_file(users, filename="data/users.json"):
    """
    Ã‰tape 6 : Enregistrer les donnÃ©es dans un JSON propre.
    """
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(users, f, indent=2, ensure_ascii=False)
    print(f"[ğŸ’¾] {len(users)} utilisateurs enregistrÃ©s dans {filename}")

# Point d'entrÃ©e du script
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Extraction utilisateurs GitHub")
    parser.add_argument("--max-users", type=int, default=60, help="Nombre maximum d'utilisateurs Ã  rÃ©cupÃ©rer")
    args = parser.parse_args()

    print(f"[ğŸš€] DÃ©but de l'extraction pour {args.max_users} utilisateurs...")
    users = extract_users(args.max_users)
    save_users_to_file(users)
